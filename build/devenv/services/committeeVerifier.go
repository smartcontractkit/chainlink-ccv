package services

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"time"

	"github.com/BurntSushi/toml"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/go-connections/nat"
	"github.com/testcontainers/testcontainers-go"
	"github.com/testcontainers/testcontainers-go/modules/postgres"
	"github.com/testcontainers/testcontainers-go/wait"

	chainsel "github.com/smartcontractkit/chain-selectors"
	bootstrap "github.com/smartcontractkit/chainlink-ccv/bootstrap"
	devenvcanton "github.com/smartcontractkit/chainlink-ccv/devenv/canton"
	"github.com/smartcontractkit/chainlink-ccv/devenv/internal/util"
	"github.com/smartcontractkit/chainlink-ccv/devenv/jobs"
	ccvblockchain "github.com/smartcontractkit/chainlink-ccv/integration/pkg/blockchain"
	"github.com/smartcontractkit/chainlink-ccv/integration/pkg/sourcereader/canton"
	"github.com/smartcontractkit/chainlink-ccv/verifier/commit"
	"github.com/smartcontractkit/chainlink-testing-framework/framework"
	"github.com/smartcontractkit/chainlink-testing-framework/framework/components/blockchain"
)

const (
	DefaultVerifierName   = "verifier"
	DefaultVerifierDBName = "verifier-db"
	DefaultVerifierImage  = "verifier:dev"
	DefaultVerifierPort   = 8100
	DefaultVerifierDBPort = 8432
	DefaultVerifierMode   = Standalone

	DefaultVerifierDBImage = "postgres:16-alpine"
)

var DefaultVerifierDBConnectionString = fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
	DefaultVerifierName, DefaultVerifierName, DefaultVerifierDBPort, DefaultVerifierName)

type VerifierDBInput struct {
	Image string `toml:"image"`
	Name  string `toml:"name"`
	Port  int    `toml:"port"`
}

type VerifierEnvConfig struct {
	AggregatorAPIKey    string `toml:"aggregator_api_key"`
	AggregatorSecretKey string `toml:"aggregator_secret_key"`
}

type VerifierInput struct {
	Mode           Mode               `toml:"mode"`
	DB             *VerifierDBInput   `toml:"db"`
	Out            *VerifierOutput    `toml:"out"`
	Image          string             `toml:"image"`
	SourceCodePath string             `toml:"source_code_path"`
	RootPath       string             `toml:"root_path"`
	ContainerName  string             `toml:"container_name"`
	NOPAlias       string             `toml:"nop_alias"`
	Port           int                `toml:"port"`
	UseCache       bool               `toml:"use_cache"`
	Env            *VerifierEnvConfig `toml:"env"`
	CommitteeName  string             `toml:"committee_name"`
	NodeIndex      int                `toml:"node_index"`

	Bootstrap *BootstrapInput `toml:"bootstrap"`

	// CantonConfigs is the map of chain selectors to canton configurations to pass onto the verifier,
	// only used in standalone mode and if Canton is enabled.
	// Note that the full party ID (name + hex) is not expected in the TOML config,
	// just the expected party name.
	// The full party ID is hydrated from the blockchain output after the Canton participant is available.
	CantonConfigs map[string]commit.CantonConfig `toml:"canton_configs"`

	// DisableFinalityCheckers is a list of chain selectors for which the finality violation checker should be disabled.
	// The chain selectors are formatted as strings of the chain selector.
	DisableFinalityCheckers []string `toml:"disable_finality_checkers"`

	// SigningKeyPublic is the public key used for on-chain committee configuration.
	SigningKeyPublic string `toml:"signing_key_public"`

	// SigningAddress is the address corresponding to the signing key.
	SigningAddress string `toml:"signing_address"`

	// TLSCACertFile is the path to the CA certificate file for TLS verification.
	TLSCACertFile string `toml:"-"`

	// InsecureAggregatorConnection disables TLS for the aggregator gRPC connection.
	InsecureAggregatorConnection bool `toml:"insecure_aggregator_connection"`

	// AggregatorOutput is optionally set to automatically obtain credentials.
	AggregatorOutput *AggregatorOutput `toml:"-"`

	// GeneratedConfig is the verifier configuration TOML generated by the changeset.
	// This is used in standalone mode. For CL mode, job specs are submitted directly.
	GeneratedConfig string `toml:"-"`
}

// GenerateConfigWithBlockchainInfos combines the pre-generated config with blockchain infos
// for standalone mode deployment.
func (v *VerifierInput) GenerateConfigWithBlockchainInfos(blockchainInfos map[string]*ccvblockchain.Info) (string, []byte, error) {
	if v.GeneratedConfig == "" {
		return "", nil, fmt.Errorf("GeneratedConfig is empty - must be set from changeset output")
	}

	// Parse the generated config
	var baseConfig commit.Config
	if _, err := toml.Decode(v.GeneratedConfig, &baseConfig); err != nil {
		return "", nil, fmt.Errorf("failed to parse generated config: %w", err)
	}

	// Apply canton config if provided.
	// Note: CantonConfigs requires runtime hydration (e.g., full party IDs from Canton participant),
	// so it must be applied here rather than in the changeset generation process.
	if v.CantonConfigs != nil {
		baseConfig.CantonConfigs = v.CantonConfigs
	}

	// Wrap with blockchain infos for standalone mode
	config := commit.ConfigWithBlockchainInfos{
		Config:          baseConfig,
		BlockchainInfos: blockchainInfos,
	}

	cfg, err := toml.Marshal(config)
	if err != nil {
		return "", nil, fmt.Errorf("failed to marshal verifier config to TOML: %w", err)
	}

	return config.VerifierID, cfg, nil
}

type VerifierOutput struct {
	VerifierID         string `toml:"verifier_id"`
	ContainerName      string `toml:"container_name"`
	ExternalHTTPURL    string `toml:"http_url"`
	InternalHTTPURL    string `toml:"internal_http_url"`
	DBURL              string `toml:"db_url"`
	DBConnectionString string `toml:"db_connection_string"`
	UseCache           bool   `toml:"use_cache"`

	// Bootstrap DB outputs
	BootstrapDBURL              string `toml:"bootstrap_db_url"`
	BootstrapDBConnectionString string `toml:"bootstrap_db_connection_string"`
	BootstrapCSAKey             string `toml:"bootstrap_csa_key"`
}

func ApplyVerifierDefaults(in VerifierInput) VerifierInput {
	if in.Image == "" {
		in.Image = DefaultVerifierImage
	}
	if in.Port == 0 {
		in.Port = DefaultVerifierPort
	}
	if in.ContainerName == "" {
		in.ContainerName = DefaultVerifierName
	}
	if in.DB == nil {
		in.DB = &VerifierDBInput{
			Image: DefaultVerifierDBImage,
			Name:  DefaultVerifierDBName,
			Port:  DefaultVerifierDBPort,
		}
	}
	if in.Mode == "" {
		in.Mode = DefaultVerifierMode
	}
	if in.Bootstrap == nil {
		def := ApplyBootstrapDefaults(BootstrapInput{})
		in.Bootstrap = &def
	} else {
		def := ApplyBootstrapDefaults(*in.Bootstrap)
		in.Bootstrap = &def
	}
	return in
}

// hydrateCantonConfig hydrates the canton config with the full party ID for the CCIPOwnerParty.
func hydrateCantonConfig(in *VerifierInput, outputs []*blockchain.Output) error {
	for _, output := range outputs {
		if output.Family != chainsel.FamilyCanton {
			continue
		}

		chainDetails, err := chainsel.GetChainDetailsByChainIDAndFamily(output.ChainID, output.Family)
		if err != nil {
			return fmt.Errorf("failed to get chain details for chain %s, family %s: %w", output.ChainID, output.Family, err)
		}

		strSelector := strconv.FormatUint(chainDetails.ChainSelector, 10)
		cantonConfig, ok := in.CantonConfigs[strSelector]
		if !ok {
			return fmt.Errorf("no canton config found for chain %s, please update the config appropriately if you're using canton", strSelector)
		}
		if cantonConfig.ReaderConfig.CCIPOwnerParty == "" {
			return fmt.Errorf("CCIPOwnerParty is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}
		if cantonConfig.ReaderConfig.CCIPMessageSentTemplateID == "" {
			return fmt.Errorf("CCIPMessageSentTemplateID is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}

		// Get the full party ID (name + hex id) from the canton participant.
		// TODO: how to support multiple participants?
		grpcURL := output.NetworkSpecificData.CantonEndpoints.Participants[0].GRPCLedgerAPIURL
		jwt := output.NetworkSpecificData.CantonEndpoints.Participants[0].JWT
		if grpcURL == "" || jwt == "" {
			return fmt.Errorf("GRPC ledger API URL or JWT is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}

		authority := grpcURL
		if idx := strings.LastIndex(authority, ":"); idx != -1 {
			authority = authority[:idx]
		}

		helper, err := devenvcanton.NewHelperFromBlockchainInput(context.Background(), grpcURL, jwt)
		if err != nil {
			return fmt.Errorf("failed to create helper for chain %s: %w", strSelector, err)
		}
		partyDetails, err := helper.ListKnownParties(context.Background())
		if err != nil {
			return fmt.Errorf("failed to list known parties for chain %s: %w", strSelector, err)
		}

		// find the party that starts with the prefix that is listed in the canton config.
		var found bool
		for _, partyDetail := range partyDetails {
			if strings.HasPrefix(partyDetail.GetParty(), cantonConfig.ReaderConfig.CCIPOwnerParty) {
				in.CantonConfigs[strSelector] = commit.CantonConfig{
					ReaderConfig: canton.ReaderConfig{
						CCIPOwnerParty:            partyDetail.GetParty(),
						CCIPMessageSentTemplateID: cantonConfig.ReaderConfig.CCIPMessageSentTemplateID,
						Authority:                 authority,
					},
				}
				found = true
				break
			}
		}
		if !found {
			return fmt.Errorf("expected CCIPOwnerParty %s not found for canton chain %s, please update the config appropriately if you're using canton", cantonConfig.ReaderConfig.CCIPOwnerParty, strSelector)
		}
	}

	return nil
}

func NewVerifier(in *VerifierInput, outputs []*blockchain.Output, jdInfra *jobs.JDInfrastructure) (*VerifierOutput, error) {
	if in == nil {
		return nil, nil
	}
	if in.Out != nil && in.Out.UseCache {
		return in.Out, nil
	}
	ctx := context.Background()

	if jdInfra == nil {
		return nil, fmt.Errorf("JD infrastructure is not set")
	}

	// Get the JD server CSA public key
	jdCSAKey, err := jobs.GetJDCSAPublicKey(ctx, jdInfra.OffchainClient)
	if err != nil {
		return nil, fmt.Errorf("failed to get JD server CSA public key: %w", err)
	}

	p, err := CwdSourcePath(in.SourceCodePath)
	if err != nil {
		return in.Out, err
	}

	// Generate blockchain infos for standalone mode
	// blockchainInfos, err := ConvertBlockchainOutputsToInfo(outputs)
	// if err != nil {
	// 	return in.Out, fmt.Errorf("failed to convert blockchain outputs to infos: %w", err)
	// }

	// Create a temporary file containing the bootstrap init script.
	// This is so that we have two databases created in the database server container, one for the verifier and one for the bootstrap.
	bootstrapInitScriptPath, err := CreateBootstrapDBInitScriptFile()
	if err != nil {
		return nil, fmt.Errorf("failed to create bootstrap init script file: %w", err)
	}

	/* Database */
	_, err = postgres.Run(ctx,
		in.DB.Image,
		testcontainers.WithName(in.DB.Name),
		postgres.WithDatabase(in.ContainerName),
		postgres.WithUsername(in.ContainerName),
		postgres.WithPassword(in.ContainerName),
		postgres.WithInitScripts(bootstrapInitScriptPath),
		testcontainers.CustomizeRequest(testcontainers.GenericContainerRequest{
			ContainerRequest: testcontainers.ContainerRequest{
				Name:         in.DB.Name,
				ExposedPorts: []string{"5432/tcp"},
				Networks:     []string{framework.DefaultNetworkName},
				NetworkAliases: map[string][]string{
					framework.DefaultNetworkName: {in.DB.Name},
				},
				Labels: framework.DefaultTCLabels(),
				HostConfigModifier: func(h *container.HostConfig) {
					h.PortBindings = nat.PortMap{
						"5432/tcp": []nat.PortBinding{
							{HostPort: strconv.Itoa(in.DB.Port)},
						},
					}
				},
				WaitingFor: wait.ForAll(
					wait.ForLog("database system is ready to accept connections"),
					wait.ForListeningPort("5432/tcp"),
				),
			},
		}),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create database: %w", err)
	}

	// Update bootstrap config w/ the database and JD info.
	// TODO: make this easier? All standalone setups will have to do the same thing.
	in.Bootstrap.DB.URL = fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
		in.ContainerName, in.ContainerName, in.DB.Port, DefaultBootstrapDBName)
	in.Bootstrap.JD.ServerCSAPublicKey = jdCSAKey
	in.Bootstrap.JD.ServerWSRPCURL = jdInfra.JDOutput.InternalWSRPCUrl

	envVars := make(map[string]string)

	var apiKey, secretKey string

	if in.Env != nil && in.Env.AggregatorAPIKey != "" && in.Env.AggregatorSecretKey != "" {
		apiKey = in.Env.AggregatorAPIKey
		secretKey = in.Env.AggregatorSecretKey
	} else if in.AggregatorOutput != nil {
		creds, ok := in.AggregatorOutput.GetCredentialsForClient(in.ContainerName)
		if ok {
			apiKey = creds.APIKey
			secretKey = creds.Secret
		}
	}

	if apiKey == "" || secretKey == "" {
		return nil, fmt.Errorf("failed to get HMAC credentials for verifier %s: no credentials provided via Env or AggregatorOutput", in.ContainerName)
	}

	envVars["VERIFIER_AGGREGATOR_API_KEY"] = apiKey
	envVars["VERIFIER_AGGREGATOR_SECRET_KEY"] = secretKey

	// Database connection for chain status (internal docker network address)
	internalDBConnectionString := fmt.Sprintf("postgresql://%s:%s@%s:5432/%s?sslmode=disable",
		in.ContainerName, in.ContainerName, in.DB.Name, in.ContainerName)
	envVars["CL_DATABASE_URL"] = internalDBConnectionString

	err = hydrateCantonConfig(in, outputs)
	if err != nil {
		return nil, fmt.Errorf("failed to hydrate canton config: %w", err)
	}

	// Generate and store config file.
	bootstrapConfig, err := GenerateBootstrapConfig(*in.Bootstrap)
	if err != nil {
		return nil, fmt.Errorf("failed to generate bootstrap config: %w", err)
	}
	confDir := util.CCVConfigDir()
	bootstrapConfigFilePath := filepath.Join(confDir,
		fmt.Sprintf("bootstrap-%s-config-%d.toml", in.CommitteeName, in.NodeIndex+1))
	if err := os.WriteFile(bootstrapConfigFilePath, bootstrapConfig, 0o644); err != nil {
		return nil, fmt.Errorf("failed to write bootstrap config to file: %w", err)
	}

	/* Service */
	req := testcontainers.ContainerRequest{
		Image:    in.Image,
		Name:     in.ContainerName,
		Labels:   framework.DefaultTCLabels(),
		Networks: []string{framework.DefaultNetworkName},
		NetworkAliases: map[string][]string{
			framework.DefaultNetworkName: {in.ContainerName},
		},
		Env: envVars,
		// This is the container port, not the host port, so it can be the same across different containers.
		// TODO: use a constant here.
		ExposedPorts: []string{"8100/tcp", DefaultBootstrapListenPortTCP},
		HostConfigModifier: func(h *container.HostConfig) {
			h.PortBindings = nat.PortMap{
				// TODO: use a constant here.
				"8100/tcp": []nat.PortBinding{
					// The host port must be unique across all containers.
					{HostPort: strconv.Itoa(in.Port)},
				},
				DefaultBootstrapListenPortTCP: []nat.PortBinding{
					{HostPort: ""}, // Docker assigns a random free host port.
				},
			}
		},
		WaitingFor: wait.
			ForHTTP(bootstrap.HealthEndpoint).
			WithPort(DefaultBootstrapListenPortTCP).
			WithStartupTimeout(120 * time.Second).
			WithPollInterval(3 * time.Second),
	}

	// Mount CA cert for TLS verification if provided. Only our self-signed CA is used for now.
	if in.TLSCACertFile != "" {
		req.Files = append(req.Files, testcontainers.ContainerFile{
			HostFilePath:      in.TLSCACertFile,
			ContainerFilePath: "/etc/ssl/certs/ca-certificates.crt",
			FileMode:          0o644,
		})
	}

	// Note: identical code to aggregator.go/executor.go -- will indexer be identical as well?
	if in.SourceCodePath != "" {
		req.Mounts = testcontainers.Mounts()
		req.Mounts = append(req.Mounts, GoSourcePathMounts(in.RootPath, AppPathInsideContainer)...)
		req.Mounts = append(req.Mounts, GoCacheMounts()...)
		req.Mounts = append(req.Mounts, testcontainers.BindMount( //nolint:staticcheck // we're still using it...
			bootstrapConfigFilePath,
			bootstrap.DefaultConfigPath,
		))
		framework.L.Info().
			Str("Service", in.ContainerName).
			Str("Source", p).Msg("Using source code path, hot-reload mode")
	}

	const maxAttempts = 3
	var c testcontainers.Container
	var lastErr error

	// We need this retry loop because sometimes air will fail to start the server
	for attempt := 1; attempt <= maxAttempts; attempt++ {
		c, err = testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
			ContainerRequest: req,
			Started:          true,
		})
		if err == nil {
			break
		}

		lastErr = err
		framework.L.Warn().Err(err).Int("attempt", attempt).Msg("Container failed to start, retrying...")

		if c != nil {
			_ = c.Terminate(ctx)
		}

		if attempt < maxAttempts {
			time.Sleep(time.Duration(attempt) * 2 * time.Second)
		}
	}

	if lastErr != nil {
		return nil, fmt.Errorf("failed to start container after %d attempts: %w", maxAttempts, lastErr)
	}

	host, err := c.Host(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get container host: %w", err)
	}

	// Get the generated CSA key from the bootstrap server.
	bootstrapMapped, err := c.MappedPort(ctx, DefaultBootstrapListenPortTCP)
	if err != nil {
		return nil, fmt.Errorf("failed to get bootstrap mapped port: %w", err)
	}
	bootstrapURL := fmt.Sprintf("http://%s:%s", host, bootstrapMapped.Port())
	bootstrapCSAKey, err := GetBootstrapCSAKey(bootstrapURL)
	if err != nil {
		return nil, fmt.Errorf("failed to get bootstrap CSA key: %w", err)
	}

	return &VerifierOutput{
		ContainerName:   in.ContainerName,
		ExternalHTTPURL: fmt.Sprintf("http://%s:%d", host, in.Port),
		InternalHTTPURL: fmt.Sprintf("http://%s:%d", in.ContainerName, in.Port),
		DBConnectionString: fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
			in.ContainerName, in.ContainerName, in.DB.Port, in.ContainerName),
		BootstrapDBURL: fmt.Sprintf("http://%s:%s", host, bootstrapMapped.Port()),
		BootstrapDBConnectionString: fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
			in.ContainerName, in.ContainerName, in.DB.Port, DefaultBootstrapDBName),
		BootstrapCSAKey: bootstrapCSAKey,
	}, nil
}
