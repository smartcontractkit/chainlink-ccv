package services

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"strconv"
	"strings"
	"time"

	"github.com/docker/docker/api/types/container"
	"github.com/docker/go-connections/nat"
	"github.com/testcontainers/testcontainers-go"
	"github.com/testcontainers/testcontainers-go/modules/postgres"
	"github.com/testcontainers/testcontainers-go/wait"

	chainsel "github.com/smartcontractkit/chain-selectors"
	devenvcanton "github.com/smartcontractkit/chainlink-ccv/devenv/canton"
	"github.com/smartcontractkit/chainlink-ccv/devenv/internal/util"
	"github.com/smartcontractkit/chainlink-ccv/integration/pkg/sourcereader/canton"
	"github.com/smartcontractkit/chainlink-ccv/verifier/commit"
	"github.com/smartcontractkit/chainlink-testing-framework/framework"
	"github.com/smartcontractkit/chainlink-testing-framework/framework/components/blockchain"
)

const (
	DefaultVerifierName   = "verifier"
	DefaultVerifierDBName = "verifier-db"
	DefaultVerifierImage  = "verifier:dev"
	// DefaultVerifierPort is the external host port for the verifier's HTTP server.
	DefaultVerifierPort = 8100
	// verifierInternalPort is the port the verifier listens on inside the container (matches main.go).
	verifierInternalPort  = 8080
	DefaultVerifierDBPort = 8432
	DefaultVerifierMode   = Standalone

	DefaultVerifierDBImage = "postgres:16-alpine"
)

var DefaultVerifierDBConnectionString = fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
	DefaultVerifierName, DefaultVerifierName, DefaultVerifierDBPort, DefaultVerifierName)

type VerifierDBInput struct {
	Image string `toml:"image"`
	Name  string `toml:"name"`
	Port  int    `toml:"port"`
}

type VerifierEnvConfig struct {
	AggregatorAPIKey    string `toml:"aggregator_api_key"`
	AggregatorSecretKey string `toml:"aggregator_secret_key"`
}

type VerifierInput struct {
	// Mode specifies how the verifier is deployed:
	// - Standalone: runs as a separate container, connects to JD for job proposals
	// - CL: runs inside a Chainlink node
	Mode Mode `toml:"mode"`

	DB             *VerifierDBInput   `toml:"db"`
	Out            *VerifierOutput    `toml:"out"`
	Image          string             `toml:"image"`
	SourceCodePath string             `toml:"source_code_path"`
	RootPath       string             `toml:"root_path"`
	ContainerName  string             `toml:"container_name"`
	NOPAlias       string             `toml:"nop_alias"`
	Port           int                `toml:"port"`
	UseCache       bool               `toml:"use_cache"`
	Env            *VerifierEnvConfig `toml:"env"`
	CommitteeName  string             `toml:"committee_name"`
	NodeIndex      int                `toml:"node_index"`

	// CantonConfigs is the map of chain selectors to canton configurations to pass onto the verifier,
	// only used in standalone mode and if Canton is enabled.
	// Note that the full party ID (name + hex) is not expected in the TOML config,
	// just the expected party name.
	// The full party ID is hydrated from the blockchain output after the Canton participant is available.
	CantonConfigs map[string]commit.CantonConfig `toml:"canton_configs"`

	// DisableFinalityCheckers is a list of chain selectors for which the finality violation checker should be disabled.
	// The chain selectors are formatted as strings of the chain selector.
	DisableFinalityCheckers []string `toml:"disable_finality_checkers"`

	// KeystorePassword is the password for the keystore encryption.
	// If empty, defaults to a devenv-only password.
	KeystorePassword string `toml:"keystore_password"`

	// TLSCACertFile is the path to the CA certificate file for TLS verification.
	TLSCACertFile string `toml:"-"`

	// InsecureAggregatorConnection disables TLS for the aggregator gRPC connection.
	InsecureAggregatorConnection bool `toml:"insecure_aggregator_connection"`

	// AggregatorOutput is optionally set to automatically obtain credentials.
	AggregatorOutput *AggregatorOutput `toml:"-"`

	// JD configuration (required)
	JDWSRPCUrl     string `toml:"-"` // Set from JD output
	JDCSAPublicKey string `toml:"-"` // Set from JD output

	// GeneratedConfig is the verifier configuration TOML generated by the changeset.
	// This is used to propose the job to the verifier via JD.
	GeneratedConfig string `toml:"-"`
}

type VerifierOutput struct {
	VerifierID         string `toml:"verifier_id"`
	ContainerName      string `toml:"container_name"`
	ExternalHTTPURL    string `toml:"http_url"`
	InternalHTTPURL    string `toml:"internal_http_url"`
	DBURL              string `toml:"db_url"`
	DBConnectionString string `toml:"db_connection_string"`
	UseCache           bool   `toml:"use_cache"`
	// SigningAddress is the Ethereum address derived from the verifier's generated signing key.
	// This is discovered by querying the /info endpoint after the verifier starts.
	SigningAddress string `toml:"signing_address"`
	// CSAPublicKey is the verifier's CSA public key for JD authentication.
	// This is discovered by querying the /info endpoint after the verifier starts.
	CSAPublicKey string `toml:"csa_public_key"`
	// JDNodeID is the Job Distributor node ID assigned when the verifier is registered with JD.
	// This is set after JD registration and is useful for debugging JD operations.
	JDNodeID string `toml:"jd_node_id,omitempty"`
}

func ApplyVerifierDefaults(in VerifierInput) VerifierInput {
	if in.Image == "" {
		in.Image = DefaultVerifierImage
	}
	if in.Port == 0 {
		in.Port = DefaultVerifierPort
	}
	if in.ContainerName == "" {
		in.ContainerName = DefaultVerifierName
	}
	if in.DB == nil {
		in.DB = &VerifierDBInput{
			Image: DefaultVerifierDBImage,
			Name:  DefaultVerifierDBName,
			Port:  DefaultVerifierDBPort,
		}
	}
	if in.Mode == "" {
		in.Mode = DefaultVerifierMode
	}
	return in
}

// hydrateCantonConfig hydrates the canton config with the full party ID for the CCIPOwnerParty.
func hydrateCantonConfig(in *VerifierInput, outputs []*blockchain.Output) error {
	for _, output := range outputs {
		if output.Family != chainsel.FamilyCanton {
			continue
		}

		chainDetails, err := chainsel.GetChainDetailsByChainIDAndFamily(output.ChainID, output.Family)
		if err != nil {
			return fmt.Errorf("failed to get chain details for chain %s, family %s: %w", output.ChainID, output.Family, err)
		}

		strSelector := strconv.FormatUint(chainDetails.ChainSelector, 10)
		cantonConfig, ok := in.CantonConfigs[strSelector]
		if !ok {
			return fmt.Errorf("no canton config found for chain %s, please update the config appropriately if you're using canton", strSelector)
		}
		if cantonConfig.ReaderConfig.CCIPOwnerParty == "" {
			return fmt.Errorf("CCIPOwnerParty is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}
		if cantonConfig.ReaderConfig.CCIPMessageSentTemplateID == "" {
			return fmt.Errorf("CCIPMessageSentTemplateID is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}

		// Get the full party ID (name + hex id) from the canton participant.
		// TODO: how to support multiple participants?
		grpcURL := output.NetworkSpecificData.CantonEndpoints.Participants[0].GRPCLedgerAPIURL
		jwt := output.NetworkSpecificData.CantonEndpoints.Participants[0].JWT
		if grpcURL == "" || jwt == "" {
			return fmt.Errorf("GRPC ledger API URL or JWT is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}

		authority := grpcURL
		if idx := strings.LastIndex(authority, ":"); idx != -1 {
			authority = authority[:idx]
		}

		helper, err := devenvcanton.NewHelperFromBlockchainInput(grpcURL, jwt)
		if err != nil {
			return fmt.Errorf("failed to create helper for chain %s: %w", strSelector, err)
		}
		partyDetails, err := helper.ListKnownParties(context.Background())
		if err != nil {
			return fmt.Errorf("failed to list known parties for chain %s: %w", strSelector, err)
		}

		// find the party that starts with the prefix that is listed in the canton config.
		var found bool
		for _, partyDetail := range partyDetails {
			if strings.HasPrefix(partyDetail.GetParty(), cantonConfig.ReaderConfig.CCIPOwnerParty) {
				in.CantonConfigs[strSelector] = commit.CantonConfig{
					ReaderConfig: canton.ReaderConfig{
						CCIPOwnerParty:            partyDetail.GetParty(),
						CCIPMessageSentTemplateID: cantonConfig.ReaderConfig.CCIPMessageSentTemplateID,
						Authority:                 authority,
					},
				}
				found = true
				break
			}
		}
		if !found {
			return fmt.Errorf("expected CCIPOwnerParty %s not found for canton chain %s, please update the config appropriately if you're using canton", cantonConfig.ReaderConfig.CCIPOwnerParty, strSelector)
		}
	}

	return nil
}

// NewVerifier creates and starts a new standalone verifier container.
// The verifier generates its own signing and CSA keys on startup.
// After startup, use QueryVerifierKeys to discover the generated keys.
func NewVerifier(in *VerifierInput, outputs []*blockchain.Output) (*VerifierOutput, error) {
	if in == nil {
		return nil, nil
	}
	if in.Out != nil && in.Out.UseCache {
		return in.Out, nil
	}
	ctx := context.Background()

	// Validate JD configuration
	if in.JDWSRPCUrl == "" {
		return nil, fmt.Errorf("JDWSRPCUrl is required for verifier (JD mode)")
	}
	if in.JDCSAPublicKey == "" {
		return nil, fmt.Errorf("JDCSAPublicKey is required for verifier (JD mode)")
	}

	p, err := CwdSourcePath(in.SourceCodePath)
	if err != nil {
		return in.Out, err
	}

	/* Database */
	_, err = postgres.Run(ctx,
		in.DB.Image,
		testcontainers.WithName(in.DB.Name),
		postgres.WithDatabase(in.ContainerName),
		postgres.WithUsername(in.ContainerName),
		postgres.WithPassword(in.ContainerName),
		testcontainers.CustomizeRequest(testcontainers.GenericContainerRequest{
			ContainerRequest: testcontainers.ContainerRequest{
				Name:         in.DB.Name,
				ExposedPorts: []string{"5432/tcp"},
				Networks:     []string{framework.DefaultNetworkName},
				NetworkAliases: map[string][]string{
					framework.DefaultNetworkName: {in.DB.Name},
				},
				Labels: framework.DefaultTCLabels(),
				HostConfigModifier: func(h *container.HostConfig) {
					h.PortBindings = nat.PortMap{
						"5432/tcp": []nat.PortBinding{
							{HostPort: strconv.Itoa(in.DB.Port)},
						},
					}
				},
				WaitingFor: wait.ForAll(
					wait.ForLog("database system is ready to accept connections"),
					wait.ForListeningPort("5432/tcp"),
				),
			},
		}),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create database: %w", err)
	}

	envVars := make(map[string]string)

	// Keystore configuration
	keystorePassword := in.KeystorePassword
	if keystorePassword == "" {
		keystorePassword = util.DefaultKeystorePassword
	}
	envVars["KEYSTORE_PASSWORD"] = keystorePassword
	envVars["KEYSTORE_STORAGE_NAME"] = util.DefaultKeystoreName

	// JD configuration
	envVars["JD_WSRPC_URL"] = in.JDWSRPCUrl
	envVars["JD_CSA_PUBLIC_KEY"] = in.JDCSAPublicKey

	var apiKey, secretKey string

	if in.Env != nil && in.Env.AggregatorAPIKey != "" && in.Env.AggregatorSecretKey != "" {
		apiKey = in.Env.AggregatorAPIKey
		secretKey = in.Env.AggregatorSecretKey
	} else if in.AggregatorOutput != nil {
		creds, ok := in.AggregatorOutput.GetCredentialsForClient(in.ContainerName)
		if ok {
			apiKey = creds.APIKey
			secretKey = creds.Secret
		}
	}

	if apiKey == "" || secretKey == "" {
		return nil, fmt.Errorf("failed to get HMAC credentials for verifier %s: no credentials provided via Env or AggregatorOutput", in.ContainerName)
	}

	envVars["VERIFIER_AGGREGATOR_API_KEY"] = apiKey
	envVars["VERIFIER_AGGREGATOR_SECRET_KEY"] = secretKey

	// Database connection for chain status (internal docker network address)
	internalDBConnectionString := fmt.Sprintf("postgresql://%s:%s@%s:5432/%s?sslmode=disable",
		in.ContainerName, in.ContainerName, in.DB.Name, in.ContainerName)
	envVars["CL_DATABASE_URL"] = internalDBConnectionString

	err = hydrateCantonConfig(in, outputs)
	if err != nil {
		return nil, fmt.Errorf("failed to hydrate canton config: %w", err)
	}

	/* Service */
	internalPortStr := fmt.Sprintf("%d/tcp", verifierInternalPort)
	req := testcontainers.ContainerRequest{
		Image:    in.Image,
		Name:     in.ContainerName,
		Labels:   framework.DefaultTCLabels(),
		Networks: []string{framework.DefaultNetworkName},
		NetworkAliases: map[string][]string{
			framework.DefaultNetworkName: {in.ContainerName},
		},
		Env:          envVars,
		ExposedPorts: []string{internalPortStr},
		HostConfigModifier: func(h *container.HostConfig) {
			h.PortBindings = nat.PortMap{
				nat.Port(internalPortStr): []nat.PortBinding{
					{HostPort: strconv.Itoa(in.Port)},
				},
			}
		},
		// Wait for the info server to be ready (keys generated, ready for JD connection)
		WaitingFor: wait.ForHTTP("/health").
			WithPort(nat.Port(internalPortStr)).
			WithStartupTimeout(120 * time.Second).
			WithPollInterval(3 * time.Second),
	}

	// Mount CA cert for TLS verification if provided. Only our self-signed CA is used for now.
	if in.TLSCACertFile != "" {
		req.Files = append(req.Files, testcontainers.ContainerFile{
			HostFilePath:      in.TLSCACertFile,
			ContainerFilePath: "/etc/ssl/certs/ca-certificates.crt",
			FileMode:          0o644,
		})
	}

	// Note: identical code to aggregator.go/executor.go -- will indexer be identical as well?
	if in.SourceCodePath != "" {
		req.Mounts = testcontainers.Mounts()
		req.Mounts = append(req.Mounts, GoSourcePathMounts(in.RootPath, AppPathInsideContainer)...)
		req.Mounts = append(req.Mounts, GoCacheMounts()...)
		framework.L.Info().
			Str("Service", in.ContainerName).
			Str("Source", p).Msg("Using source code path, hot-reload mode")
	}

	const maxAttempts = 3
	var c testcontainers.Container
	var lastErr error

	// We need this retry loop because sometimes air will fail to start the server
	for attempt := 1; attempt <= maxAttempts; attempt++ {
		c, err = testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
			ContainerRequest: req,
			Started:          true,
		})
		if err == nil {
			break
		}

		lastErr = err
		framework.L.Warn().Err(err).Int("attempt", attempt).Msg("Container failed to start, retrying...")

		if c != nil {
			_ = c.Terminate(ctx)
		}

		if attempt < maxAttempts {
			time.Sleep(time.Duration(attempt) * 2 * time.Second)
		}
	}

	if lastErr != nil {
		return nil, fmt.Errorf("failed to start container after %d attempts: %w", maxAttempts, lastErr)
	}

	host, err := c.Host(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get container host: %w", err)
	}

	externalURL := fmt.Sprintf("http://%s:%d", host, in.Port)

	// Query /info endpoint to discover auto-generated keys
	// The container wait already ensured /health is responding, so /info should be ready
	signingAddress, csaPublicKey, err := queryVerifierInfo(ctx, externalURL)
	if err != nil {
		return nil, fmt.Errorf("failed to query verifier keys: %w", err)
	}

	framework.L.Info().
		Str("Service", in.ContainerName).
		Str("url", externalURL).
		Str("signingAddress", signingAddress).
		Str("csaPublicKey", csaPublicKey[:16]+"...").
		Msg("Verifier container started with auto-generated keys")

	return &VerifierOutput{
		ContainerName:   in.ContainerName,
		ExternalHTTPURL: externalURL,
		InternalHTTPURL: fmt.Sprintf("http://%s:%d", in.ContainerName, verifierInternalPort),
		DBConnectionString: fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
			in.ContainerName, in.ContainerName, in.DB.Port, in.ContainerName),
		SigningAddress: signingAddress,
		CSAPublicKey:   csaPublicKey,
	}, nil
}

// queryVerifierInfo fetches the signing address and CSA public key from the verifier's /info endpoint.
func queryVerifierInfo(ctx context.Context, baseURL string) (signingAddress, csaPublicKey string, err error) {
	req, err := http.NewRequestWithContext(ctx, http.MethodGet, baseURL+"/info", nil)
	if err != nil {
		return "", "", err
	}

	resp, err := http.DefaultClient.Do(req)
	if err != nil {
		return "", "", err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return "", "", fmt.Errorf("unexpected status %d", resp.StatusCode)
	}

	var info struct {
		SigningAddress string `json:"signing_address"`
		CSAPublicKey   string `json:"csa_public_key"`
	}
	if err := json.NewDecoder(resp.Body).Decode(&info); err != nil {
		return "", "", err
	}

	if info.SigningAddress == "" || info.CSAPublicKey == "" {
		return "", "", fmt.Errorf("incomplete key info from verifier")
	}

	return info.SigningAddress, info.CSAPublicKey, nil
}
