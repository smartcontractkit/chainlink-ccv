package committeeverifier

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"time"

	"github.com/BurntSushi/toml"
	ledgerv2admin "github.com/digital-asset/dazl-client/v8/go/api/com/daml/ledger/api/v2/admin"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/go-connections/nat"
	"github.com/testcontainers/testcontainers-go"
	"github.com/testcontainers/testcontainers-go/modules/postgres"
	"github.com/testcontainers/testcontainers-go/wait"
	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials/insecure"

	chainsel "github.com/smartcontractkit/chain-selectors"
	bootstrap "github.com/smartcontractkit/chainlink-ccv/bootstrap"
	"github.com/smartcontractkit/chainlink-ccv/devenv/internal/util"
	"github.com/smartcontractkit/chainlink-ccv/devenv/jobs"
	"github.com/smartcontractkit/chainlink-ccv/devenv/services"
	ccvblockchain "github.com/smartcontractkit/chainlink-ccv/integration/pkg/blockchain"
	"github.com/smartcontractkit/chainlink-ccv/integration/pkg/sourcereader/canton"
	"github.com/smartcontractkit/chainlink-ccv/verifier/commit"
	"github.com/smartcontractkit/chainlink-testing-framework/framework"
	"github.com/smartcontractkit/chainlink-testing-framework/framework/components/blockchain"
	"github.com/smartcontractkit/go-daml/pkg/auth"
)

const (
	DefaultVerifierName    = "verifier"
	DefaultVerifierDBName  = "verifier-db"
	DefaultVerifierImage   = "verifier:dev"
	DefaultVerifierPort    = 8100
	DefaultVerifierPortTCP = "8100/tcp"
	DefaultVerifierDBPort  = 8432
	DefaultVerifierMode    = services.Standalone

	DefaultVerifierDBImage = "postgres:16-alpine"
)

var (
	DefaultVerifierDBConnectionString = fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
		DefaultVerifierName, DefaultVerifierName, DefaultVerifierDBPort, DefaultVerifierName)
	modifierPerFamily = map[string]ReqModifier{
		chainsel.FamilyEVM:    EVMModifier,
		chainsel.FamilyCanton: CantonModifier,
	}
)

type VerifierDBInput struct {
	Image string `toml:"image"`
	Name  string `toml:"name"`
	Port  int    `toml:"port"`
}

type VerifierEnvConfig struct {
	AggregatorAPIKey    string `toml:"aggregator_api_key"`
	AggregatorSecretKey string `toml:"aggregator_secret_key"`
}

type VerifierInput struct {
	Mode           services.Mode      `toml:"mode"`
	DB             *VerifierDBInput   `toml:"db"`
	Out            *VerifierOutput    `toml:"out"`
	Image          string             `toml:"image"`
	SourceCodePath string             `toml:"source_code_path"`
	RootPath       string             `toml:"root_path"`
	ContainerName  string             `toml:"container_name"`
	NOPAlias       string             `toml:"nop_alias"`
	Port           int                `toml:"port"`
	UseCache       bool               `toml:"use_cache"`
	Env            *VerifierEnvConfig `toml:"env"`
	CommitteeName  string             `toml:"committee_name"`
	NodeIndex      int                `toml:"node_index"`
	ChainFamily    string             `toml:"chain_family"`

	Bootstrap *services.BootstrapInput `toml:"bootstrap"`

	// CantonConfigs is the map of chain selectors to canton configurations to pass onto the verifier,
	// only used in standalone mode and if Canton is enabled.
	// Note that the full party ID (name + hex) is not expected in the TOML config,
	// just the expected party name.
	// The full party ID is hydrated from the blockchain output after the Canton participant is available.
	CantonConfigs canton.Config `toml:"canton_configs"`

	// DisableFinalityCheckers is a list of chain selectors for which the finality violation checker should be disabled.
	// The chain selectors are formatted as strings of the chain selector.
	DisableFinalityCheckers []string `toml:"disable_finality_checkers"`

	// TLSCACertFile is the path to the CA certificate file for TLS verification.
	TLSCACertFile string `toml:"-"`

	// InsecureAggregatorConnection disables TLS for the aggregator gRPC connection.
	InsecureAggregatorConnection bool `toml:"insecure_aggregator_connection"`

	// AggregatorOutput is optionally set to automatically obtain credentials.
	AggregatorOutput *services.AggregatorOutput `toml:"-"`

	// GeneratedConfig is the verifier configuration TOML generated by the changeset.
	// This is used in standalone mode. For CL mode, job specs are submitted directly.
	GeneratedConfig string `toml:"-"`
}

// RebuildVerifierJobSpecWithBlockchainInfos takes a job spec and rebuilds it with blockchain infos
// added to the inner config. This is needed for standalone verifiers which require blockchain
// connection information (CL nodes get this from their own chain config).
// TODO: we stick with the job spec so that there isn't special logic for standalone verifiers.
func (v *VerifierInput) RebuildVerifierJobSpecWithBlockchainInfos(jobSpec string, blockchainInfos map[string]*ccvblockchain.Info) (string, error) {
	// Parse the outer job spec first.
	var spec commit.JobSpec
	if _, err := toml.Decode(jobSpec, &spec); err != nil {
		return "", fmt.Errorf("failed to parse job spec: %w", err)
	}

	// Parse the inner config next.
	var cfg commit.Config
	if _, err := toml.Decode(spec.CommitteeVerifierConfig, &cfg); err != nil {
		return "", fmt.Errorf("failed to parse verifier config from job spec: %w", err)
	}

	// Create config with blockchain infos
	configWithInfos := commit.ConfigWithBlockchainInfos{
		Config:          cfg,
		BlockchainInfos: blockchainInfos,
	}

	// Marshal the enhanced config
	innerConfigBytes, err := toml.Marshal(configWithInfos)
	if err != nil {
		return "", fmt.Errorf("failed to marshal enhanced config: %w", err)
	}

	// Rebuild the job spec with the enhanced config
	spec.CommitteeVerifierConfig = string(innerConfigBytes)
	outerSpecBytes, err := toml.Marshal(spec)
	if err != nil {
		return "", fmt.Errorf("failed to marshal job spec: %w", err)
	}

	return string(outerSpecBytes), nil
}

type VerifierOutput struct {
	VerifierID         string `toml:"verifier_id"`
	ContainerName      string `toml:"container_name"`
	ExternalHTTPURL    string `toml:"http_url"`
	InternalHTTPURL    string `toml:"internal_http_url"`
	DBURL              string `toml:"db_url"`
	DBConnectionString string `toml:"db_connection_string"`
	UseCache           bool   `toml:"use_cache"`

	// Bootstrap DB outputs
	BootstrapDBURL              string                 `toml:"bootstrap_db_url"`
	BootstrapDBConnectionString string                 `toml:"bootstrap_db_connection_string"`
	BootstrapKeys               services.BootstrapKeys `toml:"bootstrap_keys"`

	// JDNodeID is set after the bootstrap is registered with JD.
	JDNodeID string `toml:"jd_node_id"`
}

func ApplyVerifierDefaults(in VerifierInput) VerifierInput {
	if in.Image == "" {
		in.Image = DefaultVerifierImage
	}
	if in.Port == 0 {
		in.Port = DefaultVerifierPort
	}
	if in.ContainerName == "" {
		in.ContainerName = DefaultVerifierName
	}
	if in.DB == nil {
		in.DB = &VerifierDBInput{
			Image: DefaultVerifierDBImage,
			Name:  DefaultVerifierDBName,
			Port:  DefaultVerifierDBPort,
		}
	}
	if in.Mode == "" {
		in.Mode = DefaultVerifierMode
	}
	if in.Bootstrap == nil {
		def := services.ApplyBootstrapDefaults(services.BootstrapInput{})
		in.Bootstrap = &def
	} else {
		def := services.ApplyBootstrapDefaults(*in.Bootstrap)
		in.Bootstrap = &def
	}
	if in.ChainFamily == "" {
		in.ChainFamily = chainsel.FamilyEVM
	}
	return in
}

// hydrateAndMarshalCantonConfig hydrates the canton config with the full party ID for the CCIPOwnerParty.
func hydrateAndMarshalCantonConfig(in *VerifierInput, outputs []*blockchain.Output) ([]byte, error) {
	for _, output := range outputs {
		if output.Family != chainsel.FamilyCanton {
			continue
		}

		chainDetails, err := chainsel.GetChainDetailsByChainIDAndFamily(output.ChainID, output.Family)
		if err != nil {
			return nil, fmt.Errorf("failed to get chain details for chain %s, family %s: %w", output.ChainID, output.Family, err)
		}

		strSelector := strconv.FormatUint(chainDetails.ChainSelector, 10)
		cantonConfig, ok := in.CantonConfigs.ReaderConfigs[strSelector]
		if !ok {
			return nil, fmt.Errorf("no canton config found for chain %s, please update the config appropriately if you're using canton", strSelector)
		}
		if cantonConfig.CCIPOwnerParty == "" {
			return nil, fmt.Errorf("CCIPOwnerParty is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}
		if cantonConfig.CCIPMessageSentTemplateID == "" {
			return nil, fmt.Errorf("CCIPMessageSentTemplateID is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}

		// Get the full party ID (name + hex id) from the canton participant.
		// TODO: how to support multiple participants?
		grpcURL := output.NetworkSpecificData.CantonEndpoints.Participants[0].GRPCLedgerAPIURL
		jwt := output.NetworkSpecificData.CantonEndpoints.Participants[0].JWT
		if grpcURL == "" || jwt == "" {
			return nil, fmt.Errorf("GRPC ledger API URL or JWT is not set for chain %s, please update the config appropriately if you're using canton", strSelector)
		}

		// find the party that starts with the prefix that is listed in the canton config.
		conn, err := grpc.NewClient(grpcURL, grpc.WithTransportCredentials(insecure.NewCredentials()), grpc.WithPerRPCCredentials(auth.NewBearerToken(jwt)))
		if err != nil {
			return nil, fmt.Errorf("failed to create gRPC connection: %w", err)
		}
		resp, err := ledgerv2admin.NewPartyManagementServiceClient(conn).ListKnownParties(context.Background(), &ledgerv2admin.ListKnownPartiesRequest{})
		if err != nil {
			return nil, fmt.Errorf("failed to get user: %w", err)
		}

		authority := grpcURL
		if idx := strings.LastIndex(authority, ":"); idx != -1 {
			authority = authority[:idx]
		}

		var found bool
		for _, partyDetail := range resp.PartyDetails {
			if strings.HasPrefix(partyDetail.GetParty(), cantonConfig.CCIPOwnerParty) {
				in.CantonConfigs.ReaderConfigs[strSelector] = canton.ReaderConfig{
					CCIPOwnerParty:            partyDetail.GetParty(),
					CCIPMessageSentTemplateID: cantonConfig.CCIPMessageSentTemplateID,
					Authority:                 authority,
				}
				found = true
				break
			}
		}
		if !found {
			return nil, fmt.Errorf("expected CCIPOwnerParty %s not found for canton chain %s, please update the config appropriately if you're using canton", cantonConfig.CCIPOwnerParty, strSelector)
		}
	}

	// Marshal the canton config into TOML.
	cantonConfigBytes, err := toml.Marshal(in.CantonConfigs)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal canton config: %w", err)
	}

	return cantonConfigBytes, nil
}

func NewVerifier(in *VerifierInput, outputs []*blockchain.Output, jdInfra *jobs.JDInfrastructure) (*VerifierOutput, error) {
	if in == nil {
		return nil, nil
	}
	if in.Out != nil && in.Out.UseCache {
		return in.Out, nil
	}
	ctx := context.Background()

	if jdInfra == nil {
		return nil, fmt.Errorf("JD infrastructure is not set")
	}

	// Get the JD server CSA public key
	jdCSAKey, err := jobs.GetJDCSAPublicKey(ctx, jdInfra.OffchainClient)
	if err != nil {
		return nil, fmt.Errorf("failed to get JD server CSA public key: %w", err)
	}

	p, err := services.CwdSourcePath(in.SourceCodePath)
	if err != nil {
		return in.Out, err
	}

	err = createVerifierDB(ctx, in)
	if err != nil {
		return nil, fmt.Errorf("failed to create verifier database: %w", err)
	}

	// Update bootstrap config w/ the database and JD info.
	// TODO: make this easier? All standalone setups will have to do the same thing.
	in.Bootstrap.DB.URL = fmt.Sprintf("postgresql://%s:%s@%s:5432/%s?sslmode=disable",
		in.ContainerName, in.ContainerName, in.DB.Name, services.DefaultBootstrapDBName)
	in.Bootstrap.JD.ServerCSAPublicKey = jdCSAKey
	in.Bootstrap.JD.ServerWSRPCURL = jdInfra.JDOutput.InternalWSRPCUrl

	envVars, err := getAggregatorSecrets(in)
	if err != nil {
		return nil, fmt.Errorf("failed to get aggregator secrets: %w", err)
	}

	// Database connection for chain status (internal docker network address)
	internalDBConnectionString := fmt.Sprintf("postgresql://%s:%s@%s:5432/%s?sslmode=disable",
		in.ContainerName, in.ContainerName, in.DB.Name, in.ContainerName)
	envVars["CL_DATABASE_URL"] = internalDBConnectionString

	// Generate and store config file.
	bootstrapConfig, err := services.GenerateBootstrapConfig(*in.Bootstrap)
	if err != nil {
		return nil, fmt.Errorf("failed to generate bootstrap config: %w", err)
	}
	confDir := util.CCVConfigDir()
	bootstrapConfigFilePath := filepath.Join(confDir,
		fmt.Sprintf("bootstrap-%s-config-%d.toml", in.CommitteeName, in.NodeIndex+1))
	if err := os.WriteFile(bootstrapConfigFilePath, bootstrapConfig, 0o644); err != nil {
		return nil, fmt.Errorf("failed to write bootstrap config to file: %w", err)
	}

	/* Service */
	req := testcontainers.ContainerRequest{
		Image:    in.Image,
		Name:     in.ContainerName,
		Labels:   framework.DefaultTCLabels(),
		Networks: []string{framework.DefaultNetworkName},
		NetworkAliases: map[string][]string{
			framework.DefaultNetworkName: {in.ContainerName},
		},
		Env: envVars,
		// This is the container port, not the host port, so it can be the same across different containers.
		ExposedPorts: []string{DefaultVerifierPortTCP, services.DefaultBootstrapListenPortTCP},
		HostConfigModifier: func(h *container.HostConfig) {
			h.PortBindings = nat.PortMap{
				DefaultVerifierPortTCP: []nat.PortBinding{
					// The host port must be unique across all containers.
					{HostPort: strconv.Itoa(in.Port)},
				},
				services.DefaultBootstrapListenPortTCP: []nat.PortBinding{
					{HostPort: ""}, // Docker assigns a random free host port.
				},
			}
		},
		WaitingFor: wait.
			ForHTTP(bootstrap.HealthEndpoint).
			WithPort(services.DefaultBootstrapListenPortTCP).
			WithStartupTimeout(120 * time.Second).
			WithPollInterval(3 * time.Second),
	}

	// Mount CA cert for TLS verification if provided. Only our self-signed CA is used for now.
	if in.TLSCACertFile != "" {
		req.Files = append(req.Files, testcontainers.ContainerFile{
			HostFilePath:      in.TLSCACertFile,
			ContainerFilePath: "/etc/ssl/certs/ca-certificates.crt",
			FileMode:          0o644,
		})
	}

	// Note: identical code to aggregator.go/executor.go -- will indexer be identical as well?
	if in.SourceCodePath != "" {
		req.Mounts = testcontainers.Mounts()
		req.Mounts = append(req.Mounts, services.GoSourcePathMounts(in.RootPath, services.AppPathInsideContainer)...)
		req.Mounts = append(req.Mounts, services.GoCacheMounts()...)
		req.Mounts = append(req.Mounts, testcontainers.BindMount( //nolint:staticcheck // we're still using it...
			bootstrapConfigFilePath,
			bootstrap.DefaultConfigPath,
		))
		framework.L.Info().
			Str("Service", in.ContainerName).
			Str("Source", p).Msg("Using source code path, hot-reload mode")
	}

	// Get the modifier for the chain family.
	modifier, ok := modifierPerFamily[in.ChainFamily]
	if !ok {
		return nil, fmt.Errorf("no modifier found for chain family %s", in.ChainFamily)
	}

	framework.L.Info().
		Str("Service", in.ContainerName).
		Str("ChainFamily", in.ChainFamily).
		Msg("Using modifier for chain family")

	// Modify the request.
	req, err = modifier(req, in, outputs)
	if err != nil {
		return nil, fmt.Errorf("failed to modify request: %w", err)
	}

	framework.L.Info().
		Str("Service", in.ContainerName).
		Str("ChainFamily", in.ChainFamily).
		Msg("Successfully modified request for chain family")

	const maxAttempts = 3
	var c testcontainers.Container
	var lastErr error

	// We need this retry loop because sometimes air will fail to start the server
	for attempt := 1; attempt <= maxAttempts; attempt++ {
		c, err = testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
			ContainerRequest: req,
			Started:          true,
		})
		if err == nil {
			break
		}

		lastErr = err
		framework.L.Warn().Err(err).Int("attempt", attempt).Msg("Container failed to start, retrying...")

		if c != nil {
			_ = c.Terminate(ctx)
		}

		if attempt < maxAttempts {
			time.Sleep(time.Duration(attempt) * 2 * time.Second)
		}
	}

	if lastErr != nil {
		return nil, fmt.Errorf("failed to start container after %d attempts: %w", maxAttempts, lastErr)
	}

	host, err := c.Host(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get container host: %w", err)
	}

	// Get the generated CSA key from the bootstrap server.
	bootstrapMapped, err := c.MappedPort(ctx, services.DefaultBootstrapListenPortTCP)
	if err != nil {
		return nil, fmt.Errorf("failed to get bootstrap mapped port: %w", err)
	}
	bootstrapURL := fmt.Sprintf("http://%s:%s", host, bootstrapMapped.Port())
	bootstrapKeys, err := services.GetBootstrapKeys(bootstrapURL)
	if err != nil {
		return nil, fmt.Errorf("failed to get bootstrap keys: %w", err)
	}

	return &VerifierOutput{
		ContainerName:   in.ContainerName,
		ExternalHTTPURL: fmt.Sprintf("http://%s:%d", host, in.Port),
		InternalHTTPURL: fmt.Sprintf("http://%s:%d", in.ContainerName, in.Port),
		DBConnectionString: fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
			in.ContainerName, in.ContainerName, in.DB.Port, in.ContainerName),
		BootstrapDBURL: fmt.Sprintf("http://%s:%s", host, bootstrapMapped.Port()),
		BootstrapDBConnectionString: fmt.Sprintf("postgresql://%s:%s@localhost:%d/%s?sslmode=disable",
			in.ContainerName, in.ContainerName, in.DB.Port, services.DefaultBootstrapDBName),
		BootstrapKeys: bootstrapKeys,
	}, nil
}

func getAggregatorSecrets(in *VerifierInput) (map[string]string, error) {
	envVars := make(map[string]string)
	var apiKey, secretKey string

	if in.Env != nil && in.Env.AggregatorAPIKey != "" && in.Env.AggregatorSecretKey != "" {
		apiKey = in.Env.AggregatorAPIKey
		secretKey = in.Env.AggregatorSecretKey
	} else if in.AggregatorOutput != nil {
		creds, ok := in.AggregatorOutput.GetCredentialsForClient(in.ContainerName)
		if ok {
			apiKey = creds.APIKey
			secretKey = creds.Secret
		}
	}

	if apiKey == "" || secretKey == "" {
		return nil, fmt.Errorf("failed to get HMAC credentials for verifier %s: no credentials provided via Env or AggregatorOutput", in.ContainerName)
	}

	envVars["VERIFIER_AGGREGATOR_API_KEY"] = apiKey
	envVars["VERIFIER_AGGREGATOR_SECRET_KEY"] = secretKey

	return envVars, nil
}

func createVerifierDB(ctx context.Context, in *VerifierInput) error {
	// Create a temporary file containing the bootstrap init script.
	// This is so that we have two databases created in the database server container, one for the verifier and one for the bootstrap.
	bootstrapInitScriptPath, err := services.CreateBootstrapDBInitScriptFile()
	if err != nil {
		return fmt.Errorf("failed to create bootstrap init script file: %w", err)
	}

	/* Database */
	_, err = postgres.Run(ctx,
		in.DB.Image,
		testcontainers.WithName(in.DB.Name),
		postgres.WithDatabase(in.ContainerName),
		postgres.WithUsername(in.ContainerName),
		postgres.WithPassword(in.ContainerName),
		postgres.WithInitScripts(bootstrapInitScriptPath),
		testcontainers.CustomizeRequest(testcontainers.GenericContainerRequest{
			ContainerRequest: testcontainers.ContainerRequest{
				Name:         in.DB.Name,
				ExposedPorts: []string{"5432/tcp"},
				Networks:     []string{framework.DefaultNetworkName},
				NetworkAliases: map[string][]string{
					framework.DefaultNetworkName: {in.DB.Name},
				},
				Labels: framework.DefaultTCLabels(),
				HostConfigModifier: func(h *container.HostConfig) {
					h.PortBindings = nat.PortMap{
						"5432/tcp": []nat.PortBinding{
							{HostPort: strconv.Itoa(in.DB.Port)},
						},
					}
				},
				WaitingFor: wait.ForAll(
					wait.ForLog("database system is ready to accept connections"),
					wait.ForListeningPort("5432/tcp"),
				),
			},
		}),
	)
	if err != nil {
		return fmt.Errorf("failed to create database: %w", err)
	}

	return nil
}

// ReqModifier is a function that modifies a testcontainers.ContainerRequest.
type ReqModifier func(
	req testcontainers.ContainerRequest,
	verifierInput *VerifierInput,
	outputs []*blockchain.Output,
) (testcontainers.ContainerRequest, error)

// CantonModifier is a function that modifies a testcontainers.ContainerRequest for canton.
func CantonModifier(req testcontainers.ContainerRequest, verifierInput *VerifierInput, outputs []*blockchain.Output) (testcontainers.ContainerRequest, error) {
	const (
		DefaultCantonCommitteVerifierImage = "cantoncommittee-verifier:dev"
	)

	// Usage the canton image to properly read from Canton.
	req.Image = DefaultCantonCommitteVerifierImage

	// Marshal the canton config into TOML bytes.
	cantonConfigBytes, err := hydrateAndMarshalCantonConfig(verifierInput, outputs)
	if err != nil {
		return req, fmt.Errorf("failed to hydrate and marshal canton config: %w", err)
	}

	// Save the canton config bytes to a temporary file.
	confDir := util.CCVConfigDir()
	cantonConfigFilePath := filepath.Join(confDir,
		fmt.Sprintf("canton-%s-config-%d.toml", verifierInput.CommitteeName, verifierInput.NodeIndex+1))
	if err := os.WriteFile(cantonConfigFilePath, cantonConfigBytes, 0o644); err != nil {
		return req, fmt.Errorf("failed to write canton config to file: %w", err)
	}

	// Mount the canton config file.
	req.Mounts = append(req.Mounts, testcontainers.BindMount(
		cantonConfigFilePath,
		canton.DefaultCantonConfigPath,
	))

	return req, nil
}

// EVMModifier is a function that modifies a testcontainers.ContainerRequest for EVM.
func EVMModifier(req testcontainers.ContainerRequest, verifierInput *VerifierInput, outputs []*blockchain.Output) (testcontainers.ContainerRequest, error) {
	return req, nil
}
